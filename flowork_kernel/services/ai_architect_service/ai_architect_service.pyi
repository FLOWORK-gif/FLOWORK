# This file was generated by Nuitka

# Stubs included by default
from __future__ import annotations
from base_service import BaseService
from llama_cpp import Llama
from typing import Any
from typing_extensions import Self
import json
import os
import re

LLAMA_CPP_AVAILABLE = True
LLAMA_CPP_AVAILABLE = False
class AiArchitectService(BaseService):
    def __init__(self: Self, kernel: Any, service_id: str) -> None: ...
    def _get_available_tools_prompt(self: Self) -> Any: ...
    def generate_workflow_from_prompt(self: Self, user_prompt: str) -> Any: ...


__name__ = ...



# Modules used internally, to allow implicit dependencies to be seen:
import json
import os
import re
import llama_cpp
import llama_cpp.Llama